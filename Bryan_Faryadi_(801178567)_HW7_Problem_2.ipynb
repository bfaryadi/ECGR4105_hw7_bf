{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime"
      ],
      "metadata": {
        "id": "clzHPdUp02G5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "krtgLlVKJ7pC"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, n_chans):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
        "    self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.batch_norm(out)\n",
        "    out = torch.relu(out)\n",
        "    return out + x\n",
        "\n",
        "class NetResDeep(nn.Module):\n",
        "  def __init__(self, n_chans1=32, n_blocks=10):\n",
        "    super(NetResDeep, self).__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = nn.functional.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, model, optimizer, loss_fn, train_loader):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {(loss_train / len(train_loader)):.4f}')\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Training completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "# Validation function\n",
        "def validate(model, train_loader, val_loader):\n",
        "  model.eval()\n",
        "  for name, loader in [('Training', train_loader), ('Validation', val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f'{name} accuracy: {correct / total:.4f}')"
      ],
      "metadata": {
        "id": "vRHjUsmf2vr8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                                  (0.2470, 0.2435, 0.2616))\n",
        "                             ]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.4942, 0.4851, 0.4504),\n",
        "                                                  (0.2467, 0.2429, 0.2616))\n",
        "                             ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfQnRRpS37VV",
        "outputId": "fb64e3ad-3183-4c63-efaa-876458d90b94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=True)\n",
        "\n",
        "model = NetResDeep()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 20,\n",
        "    model = model,\n",
        "    optimizer = optimizer,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t-XLPhf4I1X",
        "outputId": "c32cd890-f9f2-4078-8e89-5d60af486bb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 19:29:21.514271 Epoch 1, Training loss 2.0203\n",
            "2024-12-09 19:33:30.141019 Epoch 2, Training loss 1.6908\n",
            "2024-12-09 19:37:35.150030 Epoch 3, Training loss 1.4289\n",
            "2024-12-09 19:41:41.874685 Epoch 4, Training loss 1.2555\n",
            "2024-12-09 19:45:48.929531 Epoch 5, Training loss 1.1693\n",
            "2024-12-09 19:49:54.809520 Epoch 6, Training loss 1.1084\n",
            "2024-12-09 19:54:03.329151 Epoch 7, Training loss 1.0515\n",
            "2024-12-09 19:58:12.556815 Epoch 8, Training loss 0.9894\n",
            "2024-12-09 20:02:24.494461 Epoch 9, Training loss 0.9335\n",
            "2024-12-09 20:06:35.615963 Epoch 10, Training loss 0.8841\n",
            "2024-12-09 20:10:44.680837 Epoch 11, Training loss 0.8504\n",
            "2024-12-09 20:14:57.515843 Epoch 12, Training loss 0.8210\n",
            "2024-12-09 20:19:08.897550 Epoch 13, Training loss 0.7954\n",
            "2024-12-09 20:23:18.484789 Epoch 14, Training loss 0.7708\n",
            "2024-12-09 20:27:26.107398 Epoch 15, Training loss 0.7454\n",
            "2024-12-09 20:31:32.027186 Epoch 16, Training loss 0.7284\n",
            "2024-12-09 20:35:39.073138 Epoch 17, Training loss 0.7071\n",
            "2024-12-09 20:39:44.150031 Epoch 18, Training loss 0.6924\n",
            "2024-12-09 20:43:47.801821 Epoch 19, Training loss 0.6724\n",
            "2024-12-09 20:47:51.703794 Epoch 20, Training loss 0.6553\n",
            "Training completed in 4956.35 seconds.\n",
            "Training accuracy: 0.4493\n",
            "Validation accuracy: 0.4254\n"
          ]
        }
      ]
    }
  ]
}