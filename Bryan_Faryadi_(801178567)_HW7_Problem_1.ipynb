{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJP6VCHVMKYm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8*8*8, 32)\n",
        "    self.act3 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv2(out)))\n",
        "    out = out.view(-1, 8*8*8)\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "BGc6OjqZBO7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetB(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
        "    self.act3 = nn.ReLU()\n",
        "    self.pool3 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(4*4*4, 32)\n",
        "    self.act4 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv2(out)))\n",
        "    out = self.pool3(self.act3(self.conv3(out)))\n",
        "    out = out.view(-1, 4*4*4)\n",
        "    out = self.act4(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "F5Gcb0tTEqaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, model, optimizer, loss_fn, train_loader):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {(loss_train / len(train_loader)):.4f}')\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Training completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [('Training', train_loader), ('Validation', val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f'{name} accuracy: {correct / total}')"
      ],
      "metadata": {
        "id": "sZLWsEtoNbqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                                  (0.2470, 0.2435, 0.2616))\n",
        "                             ]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.4942, 0.4851, 0.4504),\n",
        "                                                  (0.2467, 0.2429, 0.2616))\n",
        "                             ]))"
      ],
      "metadata": {
        "id": "FMKiGE6eSuwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e226c43d-707c-4907-8071-195082103fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 20,\n",
        "    model = model,\n",
        "    optimizer = optimizer,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afIWm0DsSQCq",
        "outputId": "23c1e708-bd8e-498c-b86a-124165624915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-05 00:11:06.144361 Epoch 1, Training loss 2.1012\n",
            "2024-12-05 00:11:33.500231 Epoch 2, Training loss 1.6747\n",
            "2024-12-05 00:12:00.572168 Epoch 3, Training loss 1.4655\n",
            "2024-12-05 00:12:28.599565 Epoch 4, Training loss 1.3737\n",
            "2024-12-05 00:12:55.393625 Epoch 5, Training loss 1.3186\n",
            "2024-12-05 00:13:22.820662 Epoch 6, Training loss 1.2733\n",
            "2024-12-05 00:13:49.947979 Epoch 7, Training loss 1.2334\n",
            "2024-12-05 00:14:17.090650 Epoch 8, Training loss 1.2008\n",
            "2024-12-05 00:14:43.941199 Epoch 9, Training loss 1.1732\n",
            "2024-12-05 00:15:10.905414 Epoch 10, Training loss 1.1444\n",
            "2024-12-05 00:15:37.446729 Epoch 11, Training loss 1.1244\n",
            "2024-12-05 00:16:05.182844 Epoch 12, Training loss 1.1029\n",
            "2024-12-05 00:16:31.867827 Epoch 13, Training loss 1.0818\n",
            "2024-12-05 00:16:58.355076 Epoch 14, Training loss 1.0637\n",
            "2024-12-05 00:17:25.303852 Epoch 15, Training loss 1.0474\n",
            "2024-12-05 00:17:52.046049 Epoch 16, Training loss 1.0331\n",
            "2024-12-05 00:18:19.794149 Epoch 17, Training loss 1.0194\n",
            "2024-12-05 00:18:47.216607 Epoch 18, Training loss 1.0055\n",
            "2024-12-05 00:19:14.214081 Epoch 19, Training loss 0.9939\n",
            "2024-12-05 00:19:42.079823 Epoch 20, Training loss 0.9813\n",
            "Training completed in 544.30 seconds.\n",
            "Training accuracy: 0.65656\n",
            "Validation accuracy: 0.624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B"
      ],
      "metadata": {
        "id": "wkul58eiEj3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetB()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 20,\n",
        "    model = model,\n",
        "    optimizer = optimizer,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDiYsgLvEknA",
        "outputId": "b5865937-527e-4665-dc21-e4224faa7509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-05 00:20:34.939390 Epoch 1, Training loss 2.3031\n",
            "2024-12-05 00:21:02.044014 Epoch 2, Training loss 2.2866\n",
            "2024-12-05 00:21:29.231665 Epoch 3, Training loss 2.0317\n",
            "2024-12-05 00:21:56.330517 Epoch 4, Training loss 1.7494\n",
            "2024-12-05 00:22:24.130504 Epoch 5, Training loss 1.6074\n",
            "2024-12-05 00:22:51.601569 Epoch 6, Training loss 1.5399\n",
            "2024-12-05 00:23:20.038406 Epoch 7, Training loss 1.4970\n",
            "2024-12-05 00:23:47.527428 Epoch 8, Training loss 1.4617\n",
            "2024-12-05 00:24:14.957125 Epoch 9, Training loss 1.4298\n",
            "2024-12-05 00:24:42.364585 Epoch 10, Training loss 1.4002\n",
            "2024-12-05 00:25:10.154660 Epoch 11, Training loss 1.3742\n",
            "2024-12-05 00:25:38.101195 Epoch 12, Training loss 1.3476\n",
            "2024-12-05 00:26:05.673626 Epoch 13, Training loss 1.3254\n",
            "2024-12-05 00:26:35.204947 Epoch 14, Training loss 1.3012\n",
            "2024-12-05 00:27:03.875655 Epoch 15, Training loss 1.2809\n",
            "2024-12-05 00:27:31.522847 Epoch 16, Training loss 1.2620\n",
            "2024-12-05 00:27:58.856802 Epoch 17, Training loss 1.2465\n",
            "2024-12-05 00:28:26.615778 Epoch 18, Training loss 1.2298\n",
            "2024-12-05 00:28:53.985006 Epoch 19, Training loss 1.2161\n",
            "2024-12-05 00:29:21.226429 Epoch 20, Training loss 1.2056\n",
            "Training completed in 553.41 seconds.\n",
            "Training accuracy: 0.54786\n",
            "Validation accuracy: 0.5453\n"
          ]
        }
      ]
    }
  ]
}